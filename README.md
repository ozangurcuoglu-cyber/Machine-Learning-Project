# Machine Learning Homeworks

This repository contains my university machine learning assignments.

## Contents
- **PNN Models Assignment** ‚Äî Implementation and comparison of Probabilistic Neural Network architectures.
- **Predicting Road Accidents (Kaggle Competition)** ‚Äî End-to-end data pipeline, feature engineering, and model training for road accident prediction.

---

**Author:** Ozan G√ºrc√ºoƒülu  
**Environment:** Python, Jupyter Notebook, scikit-learn, NumPy, pandas







# Predicting Road Accidents (Kaggle Competition) --> **ML_HW_3.ipynb**

A Kaggle competition project focused on predicting the likelihood of road accidents based on environmental and traffic-related data.

## Key Steps
- Data cleaning and feature engineering
- Model selection and hyperparameter tuning
- Evaluation with ROC-AUC and F1 metrics
- Submission to Kaggle for leaderboard evaluation




# PNN Models Assignment-- --> **2022205111-dev-5.ipynb**

This notebook explores Probabilistic Neural Networks (PNN) and compares their performance with other supervised learning models.

## Highlights
- Implemented PNN using NumPy and scikit-learn.
- Compared accuracy and computational efficiency against traditional feedforward networks.
- Evaluated model performance on a benchmark dataset.



# Math482 ‚Äì Assignment 4: Derivation of Loss Functions

In this assignment, the objective is to **derive loss functions** starting from the probability distribution of a dataset and understand how different artificial intelligence models are trained under various distributional assumptions.

## üìò Purpose
The main goal is to learn how to mathematically derive a loss function using the **Gaussian (normal) distribution**, and to connect this derivation to commonly used functions in machine learning such as **MSE (Mean Squared Error)**.

Specifically, this notebook includes:
- Derivation of the loss function directly from the **negative log-likelihood** of the Gaussian distribution.  
- Implementation and comparison of its simplified form, the **MSE loss function**.  
- Integration of the derived loss into an existing neural network structure (activation, forward/backward propagation, training functions).

## üß† Learning Outcomes
By completing this notebook, you will:
- Understand the relationship between probability theory and optimization in neural networks.  
- See how assumptions on data distribution influence the form of the loss function.  
- Implement and analyze loss derivations in a Jupyter Notebook environment.

## ‚öôÔ∏è Environment
- **Language:** Python  
- **Libraries:** NumPy, Matplotlib  
- **Platform:** Jupyter Notebook

---

**Author:** Ozan G√ºrc√ºoƒülu  
**Course:** Math482 ‚Äì Machine Learning Theory  
**Date:** November 2025
